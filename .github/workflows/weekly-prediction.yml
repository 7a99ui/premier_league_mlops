name: Weekly Prediction Pipeline

on:
  schedule:
    # ExÃ©cution tous les lundis Ã  6h00 UTC (aprÃ¨s les matchs du weekend)
    - cron: '0 6 * * 1'
  workflow_dispatch:
    inputs:
      season:
        description: 'Season (e.g., 2024-2025)'
        required: false
        default: '2024-2025'

env:
  PYTHON_VERSION: '3.11'
  SEASON: ${{ github.event.inputs.season || '2024-2025' }}

jobs:
  weekly-prediction:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc-s3  # Required for DagsHub S3 storage
          
      - name: Configure DVC
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.DAGSHUB_USERNAME }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          # DVC avec S3 endpoint DagsHub
          echo "DVC configured with S3 endpoint"
          
      - name: Pull latest data from DVC
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.DAGSHUB_USERNAME }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc pull
          
      - name: Incremental data ingestion
        id: ingestion
        run: |
          echo "ðŸ”„ Fetching new matches for ${{ env.SEASON }}..."
          python src/data/ingestion.py \
            --mode incremental \
            --season ${{ env.SEASON }} \
            --output-dir data/raw
          
          # RÃ©cupÃ©rer le dernier gameweek disponible
          LATEST_GW=$(python -c "
          import pandas as pd
          from pathlib import Path
          
          results_file = Path('data/raw/${{ env.SEASON }}/results.csv')
          if results_file.exists():
              df = pd.read_csv(results_file)
              print(int(df['gameweek'].max()))
          else:
              print(1)
          ")
          
          echo "GAMEWEEK=${LATEST_GW}" >> $GITHUB_OUTPUT
          echo "Latest gameweek: ${LATEST_GW}"
            
      - name: Validate new data
        run: |
          python src/data/validation.py \
            --mode raw \
            --raw-data-dir data/raw
            
      - name: Generate features
        run: |
          python src/data/features.py \
            --seasons ${{ env.SEASON }} \
            --raw-data-dir data/raw \
            --output-dir data/processed/
      - name: Validate features
        run: |
          python src/data/validation.py \
            --mode features \
            --features-path data/processed/features.parquet
            
      - name: Prepare data
        run: |
          # Prepare data (split/scale) to generate test.parquet for Streamlit
          python src/data/prepare.py \
            --features-path data/processed/features.parquet \
            --output-dir data/processed
            
      - name: Configure MLflow
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          echo "MLflow configured"
          
      - name: Generate predictions
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          python src/models/predict.py \
            --season ${{ env.SEASON }} \
            --gameweek ${{ steps.ingestion.outputs.GAMEWEEK }} \
            --mlflow-stage Production \
            --output-dir predictions
            
      - name: Generate evolution predictions
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          python src/models/predict.py \
            --season ${{ env.SEASON }} \
            --gameweek ${{ steps.ingestion.outputs.GAMEWEEK }} \
            --evolution \
            --mlflow-stage Production \
            --output-dir predictions/evolution
            
      - name: Commit predictions and data to DVC
        run: |
          dvc add predictions/
          dvc add data/processed/
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add predictions.dvc data/processed.dvc .gitignore
          git commit -m "Update predictions and data for GW${{ steps.ingestion.outputs.GAMEWEEK }}" || echo "No changes to commit"
          
      - name: Push to DVC
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.DAGSHUB_USERNAME }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc push
          
      - name: Push to GitHub
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: ${{ github.ref }}
          
      - name: Create summary
        run: |
          echo "## ðŸ“Š Weekly Prediction Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Season**: ${{ env.SEASON }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Gameweek**: ${{ steps.ingestion.outputs.GAMEWEEK }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp**: $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Predictions generated and deployed successfully!" >> $GITHUB_STEP_SUMMARY
          
      - name: Notify on failure
        if: failure()
        run: |
          echo "âš ï¸ Pipeline failed - check logs above for details" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Common issues:" >> $GITHUB_STEP_SUMMARY
          echo "- DVC authentication: Check AWS credentials" >> $GITHUB_STEP_SUMMARY
          echo "- MLflow connection: Verify MLFLOW_TRACKING_URI" >> $GITHUB_STEP_SUMMARY
          echo "- Missing data: Ensure DVC remote has data" >> $GITHUB_STEP_SUMMARY