# ğŸ¯ Guide de Gestion des ModÃ¨les

Ce document explique comment gÃ©rer les modÃ¨les dans le projet Premier League MLOps.

## ğŸ“ Structure des ModÃ¨les

```
models/
â”œâ”€â”€ production/                    # ModÃ¨les en production
â”‚   â”œâ”€â”€ best_model_20231215_143052.joblib
â”‚   â”œâ”€â”€ model_metadata_20231215_143052.json
â”‚   â”œâ”€â”€ latest_model.joblib       # Symlink vers le dernier modÃ¨le
â”‚   â””â”€â”€ latest_metadata.json      # MÃ©tadonnÃ©es du dernier modÃ¨le
â””â”€â”€ experiments/                   # ModÃ¨les expÃ©rimentaux (optionnel)
```

---

## ğŸš€ EntraÃ®nement et Sauvegarde

### 1. Via le Script (Production)

```bash
# EntraÃ®nement complet avec sauvegarde automatique
python src/models/train.py --phase all --top-n 3

# Le meilleur modÃ¨le sera automatiquement sauvegardÃ© dans models/production/
```

**Avantages :**
- âœ… Automatique
- âœ… Reproductible
- âœ… PrÃªt pour CI/CD
- âœ… Versionnage automatique

### 2. Via le Notebook (Exploration)

```python
# Dans notebooks/04_model_training.ipynb
import joblib
from datetime import datetime

# AprÃ¨s avoir trouvÃ© le meilleur modÃ¨le
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
model_path = f'../models/production/best_model_{timestamp}.joblib'

joblib.dump(best_model, model_path)
print(f"Model saved to {model_path}")
```

**Avantages :**
- âœ… ContrÃ´le manuel
- âœ… Analyse dÃ©taillÃ©e
- âœ… Validation visuelle
- âœ… Documentation interactive

---

## ğŸ“Š Chargement des ModÃ¨les

### MÃ©thode 1 : Charger le Dernier ModÃ¨le

```python
from src.models.utils import ModelLoader

# Charger le dernier modÃ¨le
loader = ModelLoader()
model, metadata = loader.load_latest_model()

# Utiliser le modÃ¨le
predictions = model.predict(X_new)
```

### MÃ©thode 2 : Charger un ModÃ¨le SpÃ©cifique

```python
# Charger par timestamp
model, metadata = loader.load_model_by_timestamp('20231215_143052')
```

### MÃ©thode 3 : Lister et Comparer

```python
# Lister tous les modÃ¨les
loader.list_available_models()

# Comparer les performances
df = loader.compare_models()
```

---

## ğŸ¯ Faire des PrÃ©dictions

### MÃ©thode Simple

```python
from src.models.utils import ModelPredictor

# CrÃ©er un predictor (charge automatiquement le dernier modÃ¨le)
predictor = ModelPredictor()

# PrÃ©dire
predictions = predictor.predict(X_test)

# PrÃ©dire le classement final
standings = predictor.predict_final_standings(features_df)
```

### MÃ©thode avec DonnÃ©es Custom

```python
from src.models.utils import predict_from_latest_data

# PrÃ©dire Ã  partir d'un fichier
predictions = predict_from_latest_data(
    data_path='data/processed/v1/test.parquet',
    output_path='predictions/standings_2024.csv'
)
```

---

## ğŸ“ˆ Tracking avec MLflow

### Visualiser les ExpÃ©riences

```bash
# Lancer MLflow UI
mlflow ui --port 5000

# Ouvrir dans le navigateur
# http://localhost:5000
```

### Comparer les Runs

Dans l'UI MLflow :
1. SÃ©lectionner plusieurs runs
2. Cliquer sur "Compare"
3. Visualiser les mÃ©triques cÃ´te Ã  cÃ´te

### Filtrer les Runs

```python
import mlflow

# Rechercher les meilleurs modÃ¨les
runs = mlflow.search_runs(
    experiment_names=["premier-league-prediction"],
    filter_string="metrics.val_mae < 7.0",
    order_by=["metrics.val_mae ASC"]
)

print(runs[['run_id', 'metrics.val_mae', 'params.model_type']])
```

---

## ğŸ”„ Workflow RecommandÃ©

### Pour le DÃ©veloppement

1. **ExpÃ©rimentation** (Notebook)
   ```
   notebooks/04_model_training.ipynb
   â†’ Tester diffÃ©rentes approches
   â†’ Analyser les rÃ©sultats
   â†’ Sauvegarder manuellement les modÃ¨les intÃ©ressants
   ```

2. **Validation** (Script)
   ```
   python src/models/train.py --phase all
   â†’ Reproduire les meilleurs rÃ©sultats
   â†’ Sauvegarde automatique
   â†’ PrÃªt pour production
   ```

### Pour la Production

1. **EntraÃ®nement Automatique**
   ```bash
   # Dans un pipeline CI/CD ou cron job
   python src/models/train.py --phase all --top-n 3
   ```

2. **DÃ©ploiement**
   ```python
   # L'application charge automatiquement le dernier modÃ¨le
   from src.models.utils import ModelPredictor
   predictor = ModelPredictor()
   ```

3. **Monitoring**
   ```bash
   # VÃ©rifier les performances
   mlflow ui --port 5000
   ```

---

## ğŸ“‹ MÃ©tadonnÃ©es des ModÃ¨les

Chaque modÃ¨le sauvegardÃ© inclut :

```json
{
  "model_name": "ensemble_stacking",
  "timestamp": "20231215_143052",
  "metrics": {
    "val_mae": 6.75,
    "val_rmse": 8.92,
    "val_r2": 0.8745,
    "test_mae": 6.82,
    "test_rmse": 9.05,
    "test_r2": 0.8698
  },
  "model_file": "best_model_20231215_143052.joblib",
  "n_features": 35,
  "feature_names": ["current_points", "points_per_game", ...]
}
```

---

## ğŸš¨ Best Practices

### âœ… DO

- **Toujours versionner** les modÃ¨les avec timestamp
- **Tracker avec MLflow** toutes les expÃ©riences
- **Sauvegarder les mÃ©tadonnÃ©es** (features, mÃ©triques, hyperparamÃ¨tres)
- **Tester sur un test set** avant dÃ©ploiement
- **Documenter** les changements significatifs

### âŒ DON'T

- **Ne pas Ã©craser** `latest_model.joblib` manuellement
- **Ne pas dÃ©ployer** sans validation sur test set
- **Ne pas oublier** de sauvegarder les feature names
- **Ne pas ignorer** les warnings de compatibilitÃ© sklearn

---

## ğŸ”§ Troubleshooting

### Le modÃ¨le ne se charge pas

```python
# VÃ©rifier que le fichier existe
from pathlib import Path
model_path = Path('models/production/latest_model.joblib')
print(f"Exists: {model_path.exists()}")

# VÃ©rifier la version de scikit-learn
import sklearn
print(f"sklearn version: {sklearn.__version__}")
```

### Features manquantes

```python
# Comparer les features
loader = ModelLoader()
_, metadata = loader.load_latest_model()
print(f"Expected features: {metadata['feature_names']}")
print(f"Got features: {X_new.columns.tolist()}")
```

### PrÃ©dictions incohÃ©rentes

```python
# VÃ©rifier le scaling
from sklearn.preprocessing import StandardScaler
import joblib

scaler = joblib.load('data/processed/v1/scaler.joblib')
X_scaled = scaler.transform(X_new)
```

---

## ğŸ“š Exemples d'Utilisation

### Exemple 1 : PrÃ©dire le Classement de la Saison en Cours

```python
from src.models.utils import ModelPredictor
import pandas as pd

# Charger les features de la saison actuelle
current_season = pd.read_parquet('data/processed/v1/current_season.parquet')

# CrÃ©er le predictor
predictor = ModelPredictor()

# PrÃ©dire le classement final
standings = predictor.predict_final_standings(current_season)

# Afficher
print(standings[['predicted_rank', 'team', 'predicted_final_points']])
```

### Exemple 2 : Comparer Plusieurs ModÃ¨les

```python
from src.models.utils import ModelLoader

loader = ModelLoader()

# Lister tous les modÃ¨les
models_df = loader.compare_models()

# SÃ©lectionner le meilleur par test_mae
best_timestamp = models_df.loc[models_df['test_mae'].idxmin(), 'timestamp']

# Charger ce modÃ¨le
model, metadata = loader.load_model_by_timestamp(best_timestamp)
```

### Exemple 3 : Batch Predictions

```python
from src.models.utils import predict_from_latest_data
from pathlib import Path

# PrÃ©dire pour toutes les gameweeks d'une saison
season_data = Path('data/processed/v1/')

for gw in range(10, 39):
    gw_data = pd.read_parquet(season_data / f'gameweek_{gw}.parquet')
    predictions = predict_from_latest_data(
        gw_data,
        output_path=f'predictions/gw_{gw}_predictions.csv'
    )
```

---

## ğŸ¯ Prochaines Ã‰tapes

Une fois que vous avez un modÃ¨le en production :

1. **Monitoring** : Suivre les performances en temps rÃ©el
2. **Retraining** : RÃ©entraÃ®ner Ã  la fin de chaque saison
3. **A/B Testing** : Tester de nouveaux modÃ¨les vs production
4. **API** : CrÃ©er une API pour servir les prÃ©dictions
5. **Dashboard** : Visualiser les prÃ©dictions et performances

---

## ğŸ“ Support

- **Documentation** : Voir `README.md`
- **Issues** : Ouvrir une issue sur GitHub
- **MLflow** : `mlflow ui --port 5000`